{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ead93d5",
   "metadata": {},
   "source": [
    "# Uso de HDFS como sistema de Archivos distribuidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ede094",
   "metadata": {},
   "source": [
    "Dataset: https://www.ruoa.unam.mx/csv_data/more/minuto.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc6074-eefc-46dd-a254-c39949f745e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías de dask y distributed\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import dask, dask_ml\n",
    "import distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d72c3",
   "metadata": {},
   "source": [
    "Lo primero que se debe hacer es configurar nuestra conexión para que los workers con los que trabajemos sepan cómo tienen que manejar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00ccd9-4539-48ab-8edc-7aad55f1ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({'distributed.scheduler.allowed-failures': 99})\n",
    "dask.config.set({\"array.chunk-size\": \"256 MiB\"})\n",
    "dask.config.set({\"distributed.workers.memory.spill\": 0.85})\n",
    "dask.config.set({\"distributed.workers.memory.target\": 0.75})\n",
    "dask.config.set({\"distributed.workers.memory.terminate\": 0.98})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340ad51",
   "metadata": {},
   "source": [
    "## Conectarse al servidor\n",
    "\n",
    "Nos conectamos al servidor donde está montado el HDFS, esta dirección suele cambiar ya que depende de dondé se haya montado el sistema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e6d3d-6d38-4380-84c1-ec4f1c52953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client('tcp://132.248.203.7:8786')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bac7c",
   "metadata": {},
   "source": [
    "## Leer los datos\n",
    "\n",
    "Dask además de leer grandes vólumenes de datos, también puede leer una carpeta completa montada en HDFS, esto siempre y cuando todos los archivos tengan el mismo formato, es decir, que tengan las mismas columnas con el mismo tipo de dato.\n",
    "\n",
    "Por ejemplo en este caso se leerán todos los archivos csv en la carpeta user/michell21 los cuales representan las mediciones por hora del clima en Morelia desde 2015 hasta mediados del 2021, tomadas por la RUOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35562604-a681-4a55-a99a-32b54b9e2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv('hdfs://132.248.203.7:9820/user/michell21/*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aba2c7",
   "metadata": {},
   "source": [
    "Una vez leídos los datos, ser pueden realizar las mismas operaciones que se hacen en un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699d04c-83ae-49a6-94ae-ee0be3f424d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dedd85-ed61-47cf-9e64-71ecc4794bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f020ff-c2cd-416a-8117-0924fcc5c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecebdf3-d32f-4841-85b0-c3c89d95c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9593b-2cf3-4d06-8f54-aa7b79efa526",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"duration\"].count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147c730-0ebc-46e1-8570-25ca10e55f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este comando sirve para liberar la memoria de Dask, eliminando el Dataframe cargado al inicio (No borra los datos del HDFS)\n",
    "client.cancel(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01336af",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "El uso de Dask + HDFS es muy útil cuando se tiene que trabajar con grandes cantidades de datos, pero es importante hacer un  analisis sobre el uso del mismo ya que en ciertos casos este puede ser perjudicial para el procesamiento de datos. \n",
    "Para evitar que el uso de HDFS nos juegue en contra, hay que poner atención lo siguiente\n",
    "- Las características con las que cuenta nuestro equipo de computo. Si el analisis se puede ejecutar en un equipo local es preferible optar por este.\n",
    "- El peso de el(los) archivo(s) a analizar. Si es un archivo que pese varios Gigas o son decenas de archivos a procesar, es buena idea el uso de HDFS. \n",
    "- La velocidad de la conexión a internet (en caso de ser un servidor remoto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2a504",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
